**DATA PROCESSING CHALLENGE**

## ğŸ“‚ Project Overview

This repository includes four tasks:

1. **Data Preprocessing** â€“ Clean and preprocess datasets.
2. **Real-Time Data Streaming** â€“ Stream data using Kafka producerâ€“consumer setup.
3. **Incremental Data Processing (CDC)** â€“ Update models automatically as new data arrives.
4. **In-Memory Data Processing** â€“ Use Sparkâ€™s in-memory capabilities for efficient analysis.

---
## ğŸš€ Features

* Handle missing values, duplicates, and inconsistent data types
* Normalize and standardize datasets
* Perform feature engineering
* Stream real-time data with Kafka
* Implement incremental updates using Change Data Capture (CDC)
* Perform high-performance in-memory data analytics using Spark

---

## ğŸ“Š Dataset

* **Source:** [Marketing Campaign Dataset (Kaggle)](https://www.kaggle.com/datasets)
* **Path:** `data/raw/marketing_campaign.csv`
* The processed outputs will be saved in `data/processed/`.

---

## ğŸ›  Requirements

1. **Python 3.10+**
2. **Apache Spark** â€“ Install and configure Spark on your system
3. **Apache Kafka** â€“ Install Kafka and Zookeeper

**Install Python dependencies:**

```bash
pip install -r requirements.txt
```

**requirements.txt**

```
pyspark
kafka-python
pandas
numpy
scikit-learn
```

---

## âš¡ Task Details

### **Task 1: Data Preprocessing**

**Goal:** Clean and preprocess the dataset using Spark

**Steps performed:**

* Handle missing values
* Remove duplicates
* Fix data types
* Normalize / standardize
* Feature engineering

**Run:**

```bash
python preprocessing/spark_preprocessing.py
```



---

### **Task 2: Real-Time Data Streaming**

**Goal:** Stream data in real-time using Kafka

**Run steps:**

```bash
# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka Server
bin/kafka-server-start.sh config/server.properties

# Create topic
bin/kafka-topics.sh --create --topic sensor_data --bootstrap-server localhost:9092

# Run Producer
python realtime_streaming/kafka_producer.py

# Run Consumer
python realtime_streaming/kafka_consumer.py
```

---

### **Task 3: Incremental Data Processing (CDC)**

**Goal:** Simulate Change Data Capture; automatically update the model with new data

**Run steps:**

```bash
# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka Server
bin/kafka-server-start.sh config/server.properties

# Create topic
bin/kafka-topics.sh --create --topic customer_updates --bootstrap-server localhost:9092

# Run Producer
python incremental/kafka_cdc_producer.py

# Run Consumer
python incremental/kafka_cdc_consumer.py
```


---

### **Task 4: In-Memory Data Processing**

**Goal:** Use Sparkâ€™s in-memory processing to analyze large datasets efficiently

**Run:**

```bash
python in_memory/in_memory_processing.py
```

---

## ğŸ—‚ Folder Structure

```
data_processing/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ marketing_campaign.csv
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ spark_preprocessing.py
â”‚
â”œâ”€â”€ realtime_streaming/
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â””â”€â”€ kafka_consumer.py
â”‚
â”œâ”€â”€ incremental/
â”‚   â”œâ”€â”€ kafka_cdc_producer.py
â”‚   â””â”€â”€ kafka_cdc_consumer.py
â”‚
â”œâ”€â”€ in_memory/
â”‚   â””â”€â”€ in_memory_processing.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš ï¸ Notes

* Paths can be modified based on your local setup
* Each task runs independently


---

