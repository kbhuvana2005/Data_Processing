ğŸ§  Data Processing & Streaming Assessment








A comprehensive implementation of four major data engineering tasks â€” Data Preprocessing, Real-Time Streaming, Incremental Processing, and In-Memory Computation â€” using Apache Spark, Kafka, and Python (executed in WSL).

ğŸ“ Folder Structure
data_processing/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ marketing_campaign.csv
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ spark_preprocessing.py
â”‚
â”œâ”€â”€ realtime_streaming/
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â””â”€â”€ kafka_consumer.py
â”‚
â”œâ”€â”€ incremental/
â”‚   â”œâ”€â”€ kafka_cdc_producer.py
â”‚   â””â”€â”€ kafka_cdc_consumer.py
â”‚
â”œâ”€â”€ in_memory/
â”‚   â””â”€â”€ in_memory_processing.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Technologies Used
Category	Tools & Frameworks
Streaming	Apache Kafka, Zookeeper
Batch & In-Memory Processing	Apache Spark (PySpark)
Programming Language	Python 3.8+
Machine Learning	Scikit-learn, Pandas, NumPy
Environment	WSL (Ubuntu)
Development & Visualization	Jupyter Notebook
ğŸ§© Prerequisites

Ensure you have installed:

ğŸ Python 3.8+

ğŸ§° WSL with Ubuntu

â˜• Java 8+ (required by Spark)

âš™ï¸ Apache Kafka & Zookeeper (manually installed in WSL)

ğŸ’¾ 8 GB+ RAM

ğŸš€ Quick Start
1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/Data_Processing.git
cd Data_Processing

2ï¸âƒ£ Set Up the Environment
chmod +x preprocessing/setup.sh
./preprocessing/setup.sh

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Start Zookeeper & Kafka
# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka Broker
bin/kafka-server-start.sh config/server.properties

ğŸ§  Challenge Overview
<details> <summary><b>ğŸ”¹ Challenge 1 â€” Data Preprocessing (30%)</b></summary>

Location: preprocessing/

Goal: Clean and preprocess raw data using Spark.

Includes:

Handle missing values

Remove duplicates and incorrect types

Normalize/standardize numeric columns

Create engineered features

Run:

cd preprocessing
python spark_preprocessing.py


Output: Stored in data/processed/

</details>
<details> <summary><b>ğŸ”¹ Challenge 2 â€” Real-Time Data Streaming (35%)</b></summary>

Location: realtime_streaming/

Goal: Stream live data using Kafka producer and consumer.

Includes:

Kafka producer sending continuous data

Kafka consumer processing the data in real-time

Real-time transformation & analytics

Run:

cd realtime_streaming

# Terminal 1
python kafka_producer.py

# Terminal 2
python kafka_consumer.py

</details>
<details> <summary><b>ğŸ”¹ Challenge 3 â€” Incremental Data Processing (25%)</b></summary>

Location: incremental/

Goal: Simulate Change Data Capture (CDC) and perform incremental updates.

Includes:

CDC producer simulating record updates

Consumer applying incremental model updates

Auto model saving (model.pkl generated)

Run:

cd incremental
python kafka_cdc_producer.py
python kafka_cdc_consumer.py


ğŸ“Œ Note: The model.pkl file is auto-generated and not uploaded to GitHub.

</details>
<details> <summary><b>ğŸ”¹ Challenge 4 â€” In-Memory Data Processing (10%)</b></summary>

Location: in_memory/

Goal: Use Sparkâ€™s in-memory processing for faster computation.

Includes:

RDD & DataFrame-based caching

Performance benchmarking

Comparison with on-disk processing

Run:

cd in_memory
python in_memory_processing.py

</details>
âš™ï¸ Configuration
Kafka
Parameter	Value
Bootstrap Servers	localhost:9092
Zookeeper	localhost:2181
Default Topics	sensor_data, customer_updates
Spark
Parameter	Value
Master	local[*]
Driver Memory	4g
ğŸ“Š Dataset
File	Description
marketing_campaign.csv	Customer marketing dataset from Kaggle

ğŸ“‚ Stored in: data/raw/

ğŸ§ª Testing

Run all validation scripts (if available):

pytest tests/

ğŸ“ˆ Performance Metrics

Monitored across all tasks:

â±ï¸ Processing latency

âš¡ Throughput (messages/sec)

ğŸ’¾ Memory efficiency

ğŸ¯ Model performance

ğŸ§° Troubleshooting
# Check Kafka status
ps -ef | grep kafka

# Restart Zookeeper if needed
bin/zookeeper-server-start.sh config/zookeeper.properties &

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

ğŸ‘©â€ğŸ’» Author

Bhuvaneswari K
ğŸ“§ kbhuvana2005@github.com

ğŸ—“ï¸ October 2025

âœ… Submission Checklist

 Data Preprocessing with Spark

 Real-Time Streaming with Kafka

 Incremental Processing (CDC)

 In-Memory Processing

 Performance Evaluation

 Documentation

 Clean Folder Structure

Would you like me to add a GitHub banner image (for example, a blue Sparkâ€“Kafkaâ€“Python themed header)?
